# AI Knowledge Agent ğŸš€

[![Build Status](https://img.shields.io/badge/build-passing-brightgreen)](https://example.com)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python Version](https://img.shields.io/badge/python-3.10%2B-blue)](https://www.python.org/)
[![Qdrant](https://img.shields.io/badge/Qdrant-ready-orange)](https://qdrant.tech/)
[![Ollama](https://img.shields.io/badge/Ollama-GPT--OSS-lightgrey)](https://ollama.com/)

---

## ğŸ“Œ Project Overview

**AI Knowledge Agent** is a production-friendly template that combines **FastAPI**, **Qdrant**, and **Ollama (GPTâ€‘OSS)** to provide fast semantic search and Retrievalâ€‘Augmented Generation (RAG) over your document corpus. Itâ€™s built to be modular, configurable via environment variables, and easy to extend.

---

## âœ¨ Features

* ğŸ“„ **Document Upload** â€” Upload and index documents (PDF, TXT).
* ğŸ” **Semantic & Hybrid Search** â€” Retrieve relevant chunks using embedding vectors or hybrid search.
* ğŸ§  **RAG Pipeline** â€” Use retrieved context + Ollama LLM for grounded answers.
* âš¡ **FastAPI** â€” Lightweight, high-performance REST APIs with health checks.
* ğŸ›¢ **Qdrant** â€” Vector storage for efficient similarity search.
* ğŸ¤– **Ollama GPTâ€‘OSS** â€” Open-source LLM for on-premise generation.
* ğŸ§© **Modular Architecture** â€” Services & routes separated for maintainability.

---

## ğŸ› ï¸ Quick Start

1. **Clone the repo**

```bash
git clone https://github.com/your-repo/ai-knowledge-agent.git
cd ai-knowledge-agent
```

2. **Create & activate virtual environment**

```bash
python -m venv .venv
# macOS / Linux
source .venv/bin/activate
# Windows
.venv\Scripts\activate
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

4. **Create `.env`** (example below)

```env
APP_NAME="AI Knowledge Agent"
APP_VERSION="0.1.0"
APP_DESCRIPTION="Semantic search + RAG with GPT-OSS (Ollama) + Qdrant via FastAPI."

# Ollama
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=gpt-oss:20b
OLLAMA_EMBEDDINGS_MODEL=nomic-embed-text

# Qdrant
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION=knowledge_base
QDRANT_VECTOR_SIZE=768
QDRANT_DISTANCE=COSINE

# Chunking & Search
CHUNK_SIZE=512
TOP_K=8
MIN_CHUNKS=3
MIN_RELEVANCE=0.6

# Debug
DEBUG=true
```

5. **Run Qdrant (Docker)**

```bash
docker run -d --name qdrant -p 6333:6333 \
    -v ./qdrant_storage:/qdrant/storage \
    qdrant/qdrant
```

6. **Start FastAPI**

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

---

## ğŸ“¡ API Endpoints (Examples)

### Upload documents

**POST** `/api/docs/upload`

Upload `.pdf` or `.txt` files to be chunked, embedded, and indexed in Qdrant.

**curl** example:

```bash
curl -X POST "http://0.0.0.0:8000/api/docs/upload" \
  -F "file=@sample.pdf"
```

**Response (example)**:

```json
{
  "message": "File uploaded and indexed successfully",
  "file_name": "sample.pdf",
  "chunks_indexed": 12
}
```

---

### Ask a question (RAG)

**POST** `/api/ask`

Query the system; it will decide whether to respond with LLM-only or LLM+Docs (RAG).

**curl** example:

```bash
curl -X POST "http://0.0.0.0:8000/api/ask" \
  -H "Content-Type: application/json" \
  -d '{"query": "What is Retrieval-Augmented Generation?"}'
```

**Response (example)**:

```json
{
  "answer": "Retrieval-Augmented Generation (RAG) is a technique that combines semantic search with LLMs to provide more accurate answers.",
  "sources": [
    {
      "title": "RAG_Overview.pdf",
      "page": 2,
      "snippet": "RAG enhances LLMs by grounding responses in relevant documents."
    }
  ],
  "generated_by": "AI+Docs",
  "confidence": 0.93
}
```

---

## ğŸ“ Project Structure

```
ai-knowledge-agent/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py             # FastAPI entrypoint
â”‚   â”œâ”€â”€ routes/             # API endpoints
â”‚   â”œâ”€â”€ services/           # Ollama & RAG logic
â”‚   â”œâ”€â”€ db/qdrant_init.py   # Qdrant client setup
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py       # App configuration
â”‚   â”‚   â”œâ”€â”€ logger.py       # JSON logging
â”‚   â”‚   â””â”€â”€ exceptions.py   # Error handling
â”‚   â””â”€â”€ models/             # Request/response schemas
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .env
```

---

## ğŸ Troubleshooting

* **Qdrant Connection Refused**

  * Ensure Qdrant container is running: `docker ps`
  * Verify port `6333` is reachable

* **Ollama API Not Responding**

  * Confirm Ollama is installed and running locally
  * Check available models: `ollama list`

* **CORS Errors**

  * Configure `allow_origins` in `app/main.py` (restrict to trusted origins in production)

---

## ğŸš€ Roadmap

* ğŸ” Add authentication & session management
* ğŸ“¡ Streaming responses from Ollama
* ğŸ³ Provide Docker Compose for full stack deployment
* ğŸ”„ Improve retry logic & error handling
* âœ… Unit tests for services & routes

---

## âœ… License

MIT License Â© 2025

---

*Generated with â¤ï¸ â€” AI Knowledge Agent*
